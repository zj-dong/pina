<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="PINA: Learning a Personalized Implicit Neural Avatar from a Single RGB-D Video Sequence">
  <meta name="keywords" content="PINA">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PINA: Learning a Personalized Implicit Neural Avatar from a Single RGB-D Video Sequence</title>


  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">PINA: Learning a Personalized Implicit Neural Avatar from a Single RGB-D Video Sequence</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ait.ethz.ch/people/zijian/">Zijian Dong*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://ait.ethz.ch/people/chen/">Chen Guo*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://ait.ethz.ch/people/song/">Jie Song</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://ait.ethz.ch/people/xu/">Xu Chen</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="http://www.cvlibs.net/">Andreas Geiger</a><sup>2,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://ait.ethz.ch/people/hilliges/">Otmar Hilliges</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>ETH Zürich,</span>
            <span class="author-block"><sup>2</sup>University of Tübingen,</span>
            <br>
            <span class="author-block"><sup>3</sup>Max Planck Institute for Intelligent Systems, Tübingen
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://ait.ethz.ch/projects/2022/pina/downloads/main.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/oGpKUuD54Qk"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Medium Link. -->
              <!-- <span class="link-block">
                <a href="https://eth-ait.medium.com/animate-implicit-shapes-with-forward-skinning-c7ebbf355694"
                    class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-medium-m"></i>
                  </span>
                  <span>Medium</span>
                </a>
              </span> -->
              <!-- Blog Link. -->
              <!-- <span class="link-block">
                <a href="https://autonomousvision.github.io/snarf/"
                    class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-blogger-b"></i>
                  </span>
                  <span>Blog</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/zj-dong/pina"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (coming soon)</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img src="https://ait.ethz.ch/projects/2022/pina/downloads/assets/teaser2.png"  class="center"/>
      <h2 class="subtitle has-text-centered">
      
We propose PINA, a method to acquire personalized and animatable neural avatars from RGB-D videos. 
      
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          
          We present a novel method to learn Personalized Implicit Neural Avatars (PINA) from a short RGB-D sequence. This allows non-expert users to create a detailed and personalized virtual copy of themselves, which can be animated with realistic clothing deformations. PINA does not require complete scans, nor does it require a prior learned from large datasets  of  clothed  humans.   Learning  a  complete  avatar in this setting is challenging,  since only few depth observations are available, which are noisy and incomplete (i.e.only partial visibility of the body per frame). We propose a method to learn the shape and non-rigid deformations via a pose-conditioned implicit surface and a deformation field, defined in canonical space. This allows us to fuse all partial observations into a single consistent canonical representation. Fusion is formulated as a global optimization problem over the pose, shape and skinning parameters. The method can learn neural avatars from real noisy RGB-D sequences for  a  diverse  set  of  people  and  clothing  styles  and  these avatars can be animated given unseen motion sequences.
          
        </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    


    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
         <iframe width="560" height="315" src="https://www.youtube.com/embed/oGpKUuD54Qk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section" id="method">
  <div class="container is-max-desktop content">
    <h2 class="title">Method</h2>
    <img src="https://ait.ethz.ch/projects/2022/pina/downloads/assets/pipeline.png"  height="250" class="center"/>
    <p>
To reconstruct personalized avatars with realistic clothing deformations during animation, we propose to learn the shape and non-rigid surface deformations via a pose-conditioned implicit surface and a deformation field, defined in canonical space.
    </p>

    
    <h3 class="title"> Implicit Neural Avatar </h2>
    <p>
  Learning an animatable avatar from a monocular RGB-D sequence is challenging since raw depth images are noisy and only contain partial views of the body. At the core of our method lies the idea to fuse partial depth maps into a single, consistent representation and to learn
the articulation-driven deformations at the same time. To do so, we parametrize the 3D surface of clothed humans as a pose-conditioned implicit signed-distance field (SDF) and a learned deformation field in canonical space.
    </p>
    
    
    <h3 class="title">Training Process</h2>
    <p>
 Training is formulated as global optimization to jointly optimize the per-frame pose, shape and skinning fields without requiring prior knowledge extracted from large datasets. Based on our model parametrization, we transform the canonical surface points and the spatial gradient into posed space, enabling supervision via the input point cloud and its normals.
    </p>

  
</section>

<section class="section" id="result">
  <div class="container is-max-desktop content">
    <h2 class="title">Result</h2>

    <h3 class="title">Results on Real-world Data</h3>
    <p>
     Here, we show some of our results and all avatars have been learned from real-world RGB-D data
    </p>
    
    
    <div class="columns is-centered">
      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h4 class="title">Reconstruction</h4>
          <video autoplay controls muted loop height="100%">
            <source src="https://ait.ethz.ch/projects/2022/pina/downloads/assets/recon1.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h4 class="title">Animation</h4>
        <div class="columns is-centered">
          <div class="column content">
          <video autoplay controls muted loop height="100%">
            <source src="https://ait.ethz.ch/projects/2022/pina/downloads/assets/anim1.mp4"
                    type="video/mp4">
          </video>
          </div>
        </div>
      </div>
    </div>




    <div class="columns is-centered">
      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
       
          <video autoplay controls muted loop height="100%">
            <source src="https://ait.ethz.ch/projects/2022/pina/downloads/assets/recon2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
       
        <div class="columns is-centered">
          <div class="column content">
          <video autoplay controls muted loop height="100%">
            <source src="https://ait.ethz.ch/projects/2022/pina/downloads/assets/anim2.mp4"
                    type="video/mp4">
          </video>
          </div>
        </div>
      </div>
    </div>
    
        <div class="columns is-centered">
      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
       
          <video autoplay controls muted loop height="100%">
            <source src="https://ait.ethz.ch/projects/2022/pina/downloads/assets/recon3.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
       
        <div class="columns is-centered">
          <div class="column content">
          <video autoplay controls muted loop height="100%">
            <source src="https://ait.ethz.ch/projects/2022/pina/downloads/assets/anim3.mp4"
                    type="video/mp4">
          </video>
          </div>
        </div>
      </div>
    </div>
    
    
        <div class="columns is-centered">
      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
       
          <video autoplay controls muted loop height="100%">
            <source src="https://ait.ethz.ch/projects/2022/pina/downloads/assets/recon4.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
       
        <div class="columns is-centered">
          <div class="column content">
          <video autoplay controls muted loop height="100%">
            <source src="https://ait.ethz.ch/projects/2022/pina/downloads/assets/anim4.mp4"
                    type="video/mp4">
          </video>
          </div>
        </div>
      </div>
    </div>
    
    
       <div class="columns is-centered">
      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
       
          <video autoplay controls muted loop height="100%">
            <source src="https://ait.ethz.ch/projects/2022/pina/downloads/assets/recon5.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
       
        <div class="columns is-centered">
          <div class="column content">
          <video autoplay controls muted loop height="100%">
            <source src="https://ait.ethz.ch/projects/2022/pina/downloads/assets/anim5.mp4"
                    type="video/mp4">
          </video>
          </div>
        </div>
      </div>
    </div>


    <h3 class="title">Effects of Deformation Model</h3>
    <h4 class="title">Learned Skinning Weights</h4>
    <p>
      Without learned skinning weights, the deformed regions can be noisy and display visible artifacts.
    </p>
    <video poster="" id="chair-tp" autoplay controls muted loop width="100%">
      <source src="https://ait.ethz.ch/projects/2022/pina/downloads/assets/ablation1.mp4", type="video/mp4">
    </video>

   <h4 class="title">Pose-dependent Deformation</h4>
    <p>
     Without pose-dependent features, the shape network cannot represent dynamically changing surface details.
    </p>
    <video poster="" id="chair-tp" autoplay controls muted loop width="100%">
      <source src="https://ait.ethz.ch/projects/2022/pina/downloads/assets/ablation2.mp4", type="video/mp4">  
    

</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{dong2022pina,
      title={PINA: Learning a Personalized Implicit Neural Avatar from a Single RGB-D Video Sequence},
      author={Dong, Zijian and Guo, Chen and Song, Jie and Chen, Xu and Geiger, Andreas and Hilliges, Otmar},    
      journal   = {arXiv},
      year      = {2022}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://ait.ethz.ch/projects/2022/pina/downloads/main.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/zj-dong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This webpage is built with the template from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>. We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
